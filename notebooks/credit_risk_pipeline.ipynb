{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8739a2e4",
   "metadata": {},
   "source": [
    "# Credit Risk Preprocessing and Visualization (Notebook)\n",
    "\n",
    "This notebook consolidates the full preprocessing pipeline and visualization utilities for the UCI Credit Card default dataset.\n",
    "It optionally supports MPI (via mpi4py) if launched under an MPI environment for distributed preprocessing.\n",
    "\n",
    "- Input CSV: `archive/UCI_Credit_Card.csv`\n",
    "- Outputs: cleaned data, engineered features, train/test splits (CSV and Parquet) under `processed_data/`\n",
    "- Visualizations saved under `processed_data/visualizations/` or `visualizations/` as configured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eee9f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install extra packages if missing (uncomment as needed)\n",
    "# %pip install pyarrow fastparquet shap mpi4py\n",
    "# %pip install seaborn scikit-learn joblib psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcfbaf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os, time, gc, logging, warnings, json, traceback\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, Union, Optional, List, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "from multiprocessing import cpu_count\n",
    "from functools import partial\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Optional libraries\n",
    "try:\n",
    "    import shap\n",
    "except Exception:\n",
    "    shap = None\n",
    "\n",
    "try:\n",
    "    from mpi4py import MPI\n",
    "    MPI_AVAILABLE = True\n",
    "except Exception:\n",
    "    MPI_AVAILABLE = False\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('NotebookPipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51e5b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "from pathlib import Path\n",
    "BASE_DIR = Path.cwd()\n",
    "# If running from notebooks/ folder, use parent as base\n",
    "if (BASE_DIR.name == 'notebooks') or not (BASE_DIR / 'archive').exists():\n",
    "    if (BASE_DIR / '..' / 'archive').exists():\n",
    "        BASE_DIR = BASE_DIR.parent\n",
    "\n",
    "CONFIG_PATH = str(BASE_DIR / 'configs' / 'credit_preprocessing_config.json')\n",
    "DEFAULT_INPUT = str(BASE_DIR / 'archive' / 'UCI_Credit_Card.csv')\n",
    "DEFAULT_OUTPUT_DIR = str(BASE_DIR / 'processed_data')\n",
    "\n",
    "def load_config(path: str = CONFIG_PATH) -> Dict[str, Any]:\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        logger.warning(f'Could not read config at {path}: {e}. Using defaults.')\n",
    "        return {\n",
    "            'input_path': DEFAULT_INPUT,\n",
    "            'output_dir': DEFAULT_OUTPUT_DIR,\n",
    "            'preprocessing': {\n",
    "                'test_size': 0.2,\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1,\n",
    "                'memory_threshold': 0.8,\n",
    "                'save_intermediates': True,\n",
    "                'output_formats': ['csv','parquet'],\n",
    "                'chunk_size': 5000\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d26d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib as mpl  # required by some plotting funcs\n",
    "\n",
    "class CreditCardPreprocessor:\n",
    "    def __init__(self, input_path: str, output_dir: str, test_size: float = 0.2, random_state: int = 42,\n",
    "                 n_jobs: int = 1, memory_threshold: float = 0.7, save_intermediates: bool = False,\n",
    "                 output_formats: List[str] = ['csv','parquet'], chunk_size: int = 5000):\n",
    "        # Resolve paths relative to workspace root when running from notebooks/\n",
    "        cwd = Path.cwd()\n",
    "        root = cwd.parent if cwd.name == 'notebooks' else cwd\n",
    "        ip = Path(input_path)\n",
    "        if not ip.is_absolute():\n",
    "            ip = (root / ip)\n",
    "        self.input_path = str(ip)\n",
    "        out = Path(output_dir)\n",
    "        if not out.is_absolute():\n",
    "            out = (root / out)\n",
    "        self.output_dir = out\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        total_memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "        if total_memory_gb < 32 and n_jobs == -1:\n",
    "            logger.warning(f'Limited memory ({total_memory_gb:.1f} GB). Setting n_jobs=1 for stability.')\n",
    "            self.n_jobs = 1\n",
    "        else:\n",
    "            self.n_jobs = n_jobs if n_jobs > 0 else min(cpu_count(), 4)\n",
    "        self.memory_threshold = memory_threshold\n",
    "        self.save_intermediates = save_intermediates\n",
    "        self.output_formats = output_formats\n",
    "        self.chunk_size = chunk_size\n",
    "        self.raw_data = None\n",
    "        self.processed_data = None\n",
    "        self.X_train = self.X_test = self.y_train = self.y_test = None\n",
    "        self.demographic_cols = ['LIMIT_BAL','SEX','EDUCATION','MARRIAGE','AGE']\n",
    "        self.payment_history_cols = ['PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6']\n",
    "        self.bill_amount_cols = ['BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6']\n",
    "        self.payment_amount_cols = ['PAY_AMT1','PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6']\n",
    "        self.target_col = 'default.payment.next.month'\n",
    "        gc.enable(); gc.set_threshold(100,5,5)\n",
    "\n",
    "    def _check_memory_usage(self) -> None:\n",
    "        current = psutil.virtual_memory().percent / 100\n",
    "        if current > self.memory_threshold:\n",
    "            logger.warning(f'Memory usage ({current:.2f}) exceeds threshold ({self.memory_threshold:.2f})')\n",
    "            gc.collect()\n",
    "\n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        logger.info(f'Loading data from {self.input_path}')\n",
    "        dtypes = {'ID':'int32','LIMIT_BAL':'float32','SEX':'int8','EDUCATION':'int8','MARRIAGE':'int8','AGE':'int16',\n",
    "                  'default.payment.next.month':'int8'}\n",
    "        for col in self.payment_history_cols: dtypes[col] = 'int8'\n",
    "        for col in self.bill_amount_cols + self.payment_amount_cols: dtypes[col] = 'float32'\n",
    "        try:\n",
    "            file_size_mb = os.path.getsize(self.input_path) / (1024*1024)\n",
    "            if file_size_mb > 50 and self.chunk_size > 0:\n",
    "                chunks, total_rows = [], 0\n",
    "                for i, chunk in enumerate(pd.read_csv(self.input_path, dtype=dtypes, chunksize=self.chunk_size)):\n",
    "                    chunks.append(chunk); total_rows += len(chunk)\n",
    "                    if i % 5 == 0: logger.info(f'Loaded chunk {i+1}, total rows: {total_rows}')\n",
    "                    if i % 10 == 0: self._check_memory_usage()\n",
    "                self.raw_data = pd.concat(chunks, ignore_index=True); del chunks; gc.collect()\n",
    "            else:\n",
    "                self.raw_data = pd.read_csv(self.input_path, dtype=dtypes)\n",
    "            logger.info(f'Data loaded: {self.raw_data.shape}, mem ~ {self.raw_data.memory_usage().sum()/1024**2:.2f} MB')\n",
    "            self.analyze_data()\n",
    "            return self.raw_data\n",
    "        except Exception as e:\n",
    "            logger.error(f'Error loading data: {e}')\n",
    "            raise\n",
    "\n",
    "    def analyze_data(self) -> None:\n",
    "        if self.raw_data is None: return\n",
    "        mv = self.raw_data.isnull().sum()\n",
    "        if mv.sum() > 0: logger.warning(f'Missing values detected:\\n{mv[mv>0]}')\n",
    "        if self.target_col in self.raw_data.columns:\n",
    "            tgt = self.raw_data[self.target_col].value_counts(normalize=True) * 100\n",
    "            for v,p in tgt.items(): logger.info(f'Class {v}: {p:.2f}%')\n",
    "        self._check_data_quality()\n",
    "\n",
    "    def _check_data_quality(self) -> None:\n",
    "        rd = self.raw_data\n",
    "        if rd is None: return\n",
    "        if 'SEX' in rd.columns and (rd['SEX'].min() < 1 or rd['SEX'].max() > 2):\n",
    "            logger.warning(f'Invalid SEX values: {rd[\"SEX\"].unique()}')\n",
    "        if 'EDUCATION' in rd.columns and (rd['EDUCATION'].min() < 1 or rd['EDUCATION'].max() > 6):\n",
    "            logger.warning(f'Invalid EDUCATION values: {rd[\"EDUCATION\"].unique()}')\n",
    "        if 'MARRIAGE' in rd.columns and (rd['MARRIAGE'].min() < 0 or rd['MARRIAGE'].max() > 3):\n",
    "            logger.warning(f'Invalid MARRIAGE values: {rd[\"MARRIAGE\"].unique()}')\n",
    "        for col in self.bill_amount_cols:\n",
    "            if col in rd.columns and (rd[col] < 0).any():\n",
    "                logger.warning(f'Found {(rd[col] < 0).sum()} negative values in {col}')\n",
    "\n",
    "    def clean_data(self) -> pd.DataFrame:\n",
    "        if self.raw_data is None: raise ValueError('Raw data not loaded')\n",
    "        logger.info('Cleaning data...')\n",
    "        df = self.raw_data.copy()\n",
    "        if 'ID' in df.columns: df.drop('ID', axis=1, inplace=True)\n",
    "        df = self._impute_missing_values(df)\n",
    "        df = self._handle_outliers(df)\n",
    "        df = self._standardize_categorical(df)\n",
    "        if self.save_intermediates:\n",
    "            p = self.output_dir / 'cleaned_data.csv'; df.to_csv(p, index=False)\n",
    "        self.processed_data = df; return df\n",
    "\n",
    "    def _impute_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        self._check_memory_usage(); d = df.copy()\n",
    "        mv = d.isnull().sum().sum()\n",
    "        if mv == 0: return d\n",
    "        num_cols = d.select_dtypes(include=['number']).columns.tolist()\n",
    "        cat_cols = d.select_dtypes(exclude=['number']).columns.tolist()\n",
    "        if self.target_col in num_cols: num_cols.remove(self.target_col)\n",
    "        if self.target_col in cat_cols: cat_cols.remove(self.target_col)\n",
    "        if num_cols:\n",
    "            d[num_cols] = SimpleImputer(strategy='median').fit_transform(d[num_cols])\n",
    "        if cat_cols:\n",
    "            d[cat_cols] = SimpleImputer(strategy='most_frequent').fit_transform(d[cat_cols])\n",
    "        return d\n",
    "\n",
    "    def _handle_outliers(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        self._check_memory_usage(); d = df.copy()\n",
    "        num_cols = d.select_dtypes(include=['number']).columns.tolist()\n",
    "        if self.target_col in num_cols: num_cols.remove(self.target_col)\n",
    "        for col in ['SEX','EDUCATION','MARRIAGE']:\n",
    "            if col in num_cols: num_cols.remove(col)\n",
    "        def cap(col):\n",
    "            Q1, Q3 = d[col].quantile(0.25), d[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lb, ub = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
    "            return d[col].clip(lower=lb, upper=ub)\n",
    "        if self.n_jobs > 1:\n",
    "            capped = Parallel(n_jobs=self.n_jobs)(delayed(cap)(c) for c in num_cols)\n",
    "            for c, s in zip(num_cols, capped): d[c] = s\n",
    "        else:\n",
    "            for c in num_cols: d[c] = cap(c)\n",
    "        return d\n",
    "\n",
    "    def _standardize_categorical(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        d = df.copy()\n",
    "        if 'EDUCATION' in d.columns:\n",
    "            d.loc[d['EDUCATION'] >= 5, 'EDUCATION'] = 4\n",
    "        if 'MARRIAGE' in d.columns:\n",
    "            d.loc[d['MARRIAGE'] == 0, 'MARRIAGE'] = 3\n",
    "        for col in self.payment_history_cols:\n",
    "            if col in d.columns:\n",
    "                d.loc[d[col] < -2, col] = -2\n",
    "                d.loc[d[col] > 9, col] = 9\n",
    "        return d\n",
    "\n",
    "    def engineer_features(self) -> pd.DataFrame:\n",
    "        if self.processed_data is None:\n",
    "            if self.raw_data is not None:\n",
    "                self.clean_data()\n",
    "            else:\n",
    "                raise ValueError('Raw data not loaded')\n",
    "        df = self.processed_data.copy()\n",
    "        df = self._create_utilization_features(df)\n",
    "        df = self._create_payment_features(df)\n",
    "        df = self._create_demographic_features(df)\n",
    "        df = self._create_financial_stress_features(df)\n",
    "        df = self._create_trend_features(df)\n",
    "        if self.save_intermediates:\n",
    "            (self.output_dir / 'engineered_features.csv').parent.mkdir(parents=True, exist_ok=True)\n",
    "            df.to_csv(self.output_dir / 'engineered_features.csv', index=False)\n",
    "        self.processed_data = df; return df\n",
    "\n",
    "    def _create_utilization_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if not all(c in df.columns for c in self.bill_amount_cols + ['LIMIT_BAL']):\n",
    "            return df\n",
    "        util_cols = []\n",
    "        for i, bill_col in enumerate(self.bill_amount_cols, start=1):\n",
    "            col = f'UTIL_RATIO_{i}'\n",
    "            util_cols.append(col)\n",
    "            df.loc[:, col] = df[bill_col] / df['LIMIT_BAL'].replace(0, np.nan).fillna(1)\n",
    "            df.loc[:, f'UTIL_RATIO_CAPPED_{i}'] = df[col].clip(0,1)\n",
    "        df.loc[:, 'UTIL_RATIO_AVG'] = df[util_cols].mean(axis=1)\n",
    "        df.loc[:, 'UTIL_RATIO_MAX'] = df[util_cols].max(axis=1)\n",
    "        df.loc[:, 'UTIL_RATIO_MIN'] = df[util_cols].min(axis=1)\n",
    "        df.loc[:, 'UTIL_RATIO_STD'] = df[util_cols].std(axis=1)\n",
    "        recent = util_cols[:3]\n",
    "        if len(recent) == 3:\n",
    "            df.loc[:, 'UTIL_RATIO_RECENT_AVG'] = df[recent].mean(axis=1)\n",
    "        for i in range(len(util_cols)-1):\n",
    "            df.loc[:, f'UTIL_CHANGE_{i+1}_{i+2}'] = df[util_cols[i]] - df[util_cols[i+1]]\n",
    "        df.loc[:, 'HIGH_UTIL_COUNT'] = (df[util_cols] > 0.8).sum(axis=1)\n",
    "        df.loc[:, 'ZERO_UTIL_COUNT'] = (df[util_cols] == 0).sum(axis=1)\n",
    "        return df\n",
    "\n",
    "    def _create_payment_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if not all(c in df.columns for c in self.payment_history_cols):\n",
    "            return df\n",
    "        amount_cols_exist = all(c in df.columns for c in self.payment_amount_cols + self.bill_amount_cols)\n",
    "        df['DELAY_COUNT'] = (df[self.payment_history_cols] > 0).sum(axis=1)\n",
    "        df['MAX_DELAY'] = df[self.payment_history_cols].max(axis=1)\n",
    "        delay_mask = df[self.payment_history_cols] > 0\n",
    "        df['AVG_DELAY'] = df[self.payment_history_cols].where(delay_mask).mean(axis=1).fillna(0)\n",
    "        df['RECENT_DELAY_COUNT'] = (df[['PAY_0','PAY_2','PAY_3']] > 0).sum(axis=1)\n",
    "        df['PAYMENT_CONSISTENCY'] = df[self.payment_history_cols].var(axis=1)\n",
    "        df['PAYMENT_TREND'] = df['PAY_6'] - df['PAY_0']\n",
    "        if amount_cols_exist:\n",
    "            ratio_cols = []\n",
    "            for i, (pcol, bcol) in enumerate(zip(self.payment_amount_cols, self.bill_amount_cols), start=1):\n",
    "                r = f'PAYMENT_RATIO_{i}'\n",
    "                df[r] = (df[pcol] / df[bcol].replace(0, np.nan)).fillna(0)\n",
    "                df[f'PAYMENT_RATIO_CAPPED_{i}'] = df[r].clip(0,1)\n",
    "                ratio_cols.append(r)\n",
    "            df['PAYMENT_RATIO_AVG'] = df[ratio_cols].mean(axis=1)\n",
    "            df['PAYMENT_RATIO_MIN'] = df[ratio_cols].min(axis=1)\n",
    "            df['FULL_PAYMENT_COUNT'] = (df[ratio_cols] >= 0.99).sum(axis=1)\n",
    "            df['MIN_PAYMENT_COUNT'] = ((df[ratio_cols] >= 0.05) & (df[ratio_cols] < 0.1)).sum(axis=1)\n",
    "        return df\n",
    "\n",
    "    def _create_demographic_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if not all(c in df.columns for c in ['AGE','EDUCATION','MARRIAGE']): return df\n",
    "        df['AGE_GROUP'] = pd.cut(df['AGE'], bins=[0,25,35,45,55,65,100], labels=['<25','25-35','36-45','46-55','56-65','65+'])\n",
    "        edu_risk_map = {1:'Low',2:'Low',3:'Medium',4:'High'}\n",
    "        df['EDUCATION_RISK'] = df['EDUCATION'].map(edu_risk_map)\n",
    "        marriage_stability_map = {1:'Stable',2:'Medium',3:'Variable'}\n",
    "        df['MARRIAGE_STABILITY'] = df['MARRIAGE'].map(marriage_stability_map)\n",
    "        df['YOUNG_UNMARRIED'] = ((df['AGE'] < 30) & (df['MARRIAGE'] == 2)).astype(int)\n",
    "        df['AGE_RISK'] = pd.cut(df['AGE'], bins=[0,25,30,40,50,100], labels=[4,3,2,1,0]).astype(int)\n",
    "        df['EDU_RISK'] = df['EDUCATION'].map({1:0,2:1,3:2,4:3})\n",
    "        df['MARR_RISK'] = df['MARRIAGE'].map({1:0,2:1,3:2})\n",
    "        df['DEMO_RISK_SCORE'] = df['AGE_RISK'] + df['EDU_RISK'] + df['MARR_RISK']\n",
    "        return df\n",
    "\n",
    "    def _create_financial_stress_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if not all(c in df.columns for c in self.bill_amount_cols) or not all(c in df.columns for c in self.payment_amount_cols):\n",
    "            return df\n",
    "        df['AVG_PAYMENT'] = df[self.payment_amount_cols].mean(axis=1)\n",
    "        mask = df['AVG_PAYMENT'] > 0\n",
    "        df.loc[mask, 'CREDIT_TO_PAYMENT'] = df.loc[mask, 'LIMIT_BAL'] / df.loc[mask, 'AVG_PAYMENT']\n",
    "        df['CREDIT_TO_PAYMENT'] = df['CREDIT_TO_PAYMENT'].fillna(df['CREDIT_TO_PAYMENT'].median())\n",
    "        df['CREDIT_TO_PAYMENT'] = df['CREDIT_TO_PAYMENT'].clip(upper=df['CREDIT_TO_PAYMENT'].quantile(0.99))\n",
    "        df['BILL_TREND'] = df['BILL_AMT1'] - df['BILL_AMT6']\n",
    "        df['BILL_GROWTH_RATE'] = df['BILL_TREND'] / df[['BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6']].mean(axis=1)\n",
    "        df['BILL_GROWTH_RATE'].replace([np.inf,-np.inf], np.nan, inplace=True)\n",
    "        df['BILL_GROWTH_RATE'].fillna(0, inplace=True)\n",
    "        total_payments = df[self.payment_amount_cols].sum(axis=1)\n",
    "        total_bills = df[self.bill_amount_cols].sum(axis=1)\n",
    "        df['PAYMENT_TO_BILL_RATIO'] = total_payments / total_bills\n",
    "        df['PAYMENT_TO_BILL_RATIO'].replace([np.inf,-np.inf], np.nan, inplace=True)\n",
    "        df['PAYMENT_TO_BILL_RATIO'].fillna(0, inplace=True)\n",
    "        stress_cols = []\n",
    "        for i, (pcol, bcol) in enumerate(zip(self.payment_amount_cols, self.bill_amount_cols), start=1):\n",
    "            col = f'PAYMENT_STRESS_{i}'\n",
    "            df[col] = (df[pcol] < (df[bcol] * 0.05)) & (df[bcol] > 0)\n",
    "            stress_cols.append(col)\n",
    "        df['PAYMENT_STRESS_COUNT'] = df[stress_cols].sum(axis=1)\n",
    "        df['DEBT_SPIRAL'] = ((df['BILL_TREND'] > 0) & (df['PAYMENT_TO_BILL_RATIO'] < 0.1) & (df['UTIL_RATIO_AVG'] > 0.8)).astype(int)\n",
    "        df['UTIL_NORM'] = df['UTIL_RATIO_AVG'] / 1.0\n",
    "        df['DELAY_NORM'] = df['DELAY_COUNT'] / 6.0\n",
    "        df['STRESS_NORM'] = df['PAYMENT_STRESS_COUNT'] / 6.0\n",
    "        df['FINANCIAL_STRESS_SCORE'] = ((df['UTIL_NORM']*0.4) + (df['DELAY_NORM']*0.4) + (df['STRESS_NORM']*0.2)) * 10\n",
    "        return df\n",
    "\n",
    "    def _create_trend_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if not all(c in df.columns for c in self.bill_amount_cols) or not all(c in df.columns for c in self.payment_amount_cols):\n",
    "            return df\n",
    "        for i in range(1, len(self.bill_amount_cols)):\n",
    "            df[f'BILL_MOMENTUM_{i}'] = df[f'BILL_AMT{i}'] - df[f'BILL_AMT{i+1}']\n",
    "        for i in range(1, len(self.payment_amount_cols)):\n",
    "            df[f'PAYMENT_MOMENTUM_{i}'] = df[f'PAY_AMT{i}'] - df[f'PAY_AMT{i+1}']\n",
    "        bill_std = df[self.bill_amount_cols].std(axis=1); bill_mean = df[self.bill_amount_cols].mean(axis=1)\n",
    "        df['BILL_VOLATILITY'] = (bill_std / bill_mean).replace([np.inf,-np.inf], np.nan).fillna(0)\n",
    "        pay_std = df[self.payment_amount_cols].std(axis=1); pay_mean = df[self.payment_amount_cols].mean(axis=1)\n",
    "        df['PAYMENT_VOLATILITY'] = (pay_std / pay_mean).replace([np.inf,-np.inf], np.nan).fillna(0)\n",
    "        df['BILL_MA_3M'] = df[['BILL_AMT1','BILL_AMT2','BILL_AMT3']].mean(axis=1)\n",
    "        df['BILL_MA_6M'] = df[self.bill_amount_cols].mean(axis=1)\n",
    "        df['BILL_SEASONAL'] = df['BILL_AMT1'] + df['BILL_AMT4'] - df['BILL_MA_6M']\n",
    "        df['BILL_ACCEL'] = (df['BILL_AMT1'] - df['BILL_AMT2']) - (df['BILL_AMT2'] - df['BILL_AMT3'])\n",
    "        df['PAYMENT_ACCEL'] = (df['PAY_AMT1'] - df['PAY_AMT2']) - (df['PAY_AMT2'] - df['PAY_AMT3'])\n",
    "        return df\n",
    "\n",
    "    def encode_categoricals(self, df: pd.DataFrame = None) -> pd.DataFrame:\n",
    "        d = self.processed_data.copy() if df is None else df.copy()\n",
    "        cats = d.select_dtypes(include=['object','category']).columns.tolist()\n",
    "        if not cats: return d\n",
    "        for col in cats:\n",
    "            dum = pd.get_dummies(d[col], prefix=col, drop_first=True)\n",
    "            d = pd.concat([d, dum], axis=1).drop(columns=[col])\n",
    "        return d\n",
    "\n",
    "    def scale_features(self, df: pd.DataFrame = None) -> Tuple[pd.DataFrame, Optional[RobustScaler]]:\n",
    "        d = self.processed_data.copy() if df is None else df.copy()\n",
    "        y = d[self.target_col].copy() if self.target_col in d.columns else None\n",
    "        num_cols = d.select_dtypes(include=['number']).columns.tolist()\n",
    "        if self.target_col in num_cols: num_cols.remove(self.target_col)\n",
    "        if not num_cols: return d, None\n",
    "        scaler = RobustScaler(); d[num_cols] = scaler.fit_transform(d[num_cols])\n",
    "        if y is not None: d[self.target_col] = y\n",
    "        return d, scaler\n",
    "\n",
    "    def split_data(self, df: pd.DataFrame = None) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "        d = self.processed_data.copy() if df is None else df.copy()\n",
    "        if self.target_col not in d.columns:\n",
    "            raise ValueError(f\"Target column '{self.target_col}' not found in data\")\n",
    "        X, y = d.drop(self.target_col, axis=1), d[self.target_col]\n",
    "        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=self.test_size, random_state=self.random_state, stratify=y)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = Xtr, Xte, ytr, yte\n",
    "        return Xtr, Xte, ytr, yte\n",
    "\n",
    "    def save_full_processed(self) -> None:\n",
    "        if self.processed_data is None:\n",
    "            return\n",
    "        # Always save final processed dataset\n",
    "        df = self.processed_data\n",
    "        csv_path = self.output_dir / 'processed_full.csv'\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        # Save parquet when available\n",
    "        if 'parquet' in [f.lower() for f in self.output_formats]:\n",
    "            try:\n",
    "                df.to_parquet(self.output_dir / 'processed_full.parquet', index=False)\n",
    "            except Exception as e:\n",
    "                logger.warning(f'Parquet save failed for processed_full ({e}).')\n",
    "\n",
    "    def save_data(self) -> None:\n",
    "        if self.X_train is None or self.X_test is None: return\n",
    "        (self.output_dir / 'train').mkdir(exist_ok=True); (self.output_dir / 'test').mkdir(exist_ok=True)\n",
    "        for fmt in self.output_formats:\n",
    "            fmt = fmt.lower()\n",
    "            if fmt == 'csv':\n",
    "                pd.concat([self.X_train, self.y_train], axis=1).to_csv(self.output_dir / 'train' / 'train.csv', index=False)\n",
    "                pd.concat([self.X_test, self.y_test], axis=1).to_csv(self.output_dir / 'test' / 'test.csv', index=False)\n",
    "            elif fmt == 'parquet':\n",
    "                try:\n",
    "                    pd.concat([self.X_train, self.y_train], axis=1).to_parquet(self.output_dir / 'train' / 'train.parquet', index=False)\n",
    "                    pd.concat([self.X_test, self.y_test], axis=1).to_parquet(self.output_dir / 'test' / 'test.parquet', index=False)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f'Parquet save failed ({e}). Saving CSV instead.')\n",
    "                    pd.concat([self.X_train, self.y_train], axis=1).to_csv(self.output_dir / 'train' / 'train.csv', index=False)\n",
    "                    pd.concat([self.X_test, self.y_test], axis=1).to_csv(self.output_dir / 'test' / 'test.csv', index=False)\n",
    "\n",
    "    def _log_feature_importance(self) -> None:\n",
    "        if self.processed_data is None or self.target_col not in self.processed_data.columns: return\n",
    "        corrs = self.processed_data.corr(numeric_only=True)[self.target_col].sort_values(ascending=False)\n",
    "        logger.info('Top features by correlation with target:')\n",
    "        count = 0\n",
    "        for feature, corr in corrs.items():\n",
    "            if feature == self.target_col: continue\n",
    "            logger.info(f'  {feature}: {corr:.4f}')\n",
    "            count += 1\n",
    "            if count >= 10: break\n",
    "\n",
    "    def process_pipeline(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "        logger.info('Starting preprocessing pipeline...')\n",
    "        self.load_data()\n",
    "        self.clean_data()\n",
    "        self.engineer_features()\n",
    "        self.processed_data = self.encode_categoricals()\n",
    "        self.processed_data, _ = self.scale_features()\n",
    "        # Ensure full processed dataset is persisted\n",
    "        self.save_full_processed()\n",
    "        Xtr, Xte, ytr, yte = self.split_data()\n",
    "        self.save_data()\n",
    "        self._log_feature_importance()\n",
    "        return Xtr, Xte, ytr, yte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a042d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization utilities (from src/feature_visualization.py)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "class FeatureVisualization:\n",
    "    def __init__(self, data: pd.DataFrame, target_col: str = 'default.payment.next.month', output_dir: str = './visualizations', feature_groups: Optional[Dict[str, List[str]]] = None):\n",
    "        self.data = data.copy()\n",
    "        self.target_col = target_col\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        nan_count = self.data.isna().sum().sum()\n",
    "        if nan_count > 0:\n",
    "            num_cols = self.data.select_dtypes(include=['number']).columns\n",
    "            self.data[num_cols] = self.data[num_cols].fillna(self.data[num_cols].median())\n",
    "            cat_cols = self.data.select_dtypes(exclude=['number']).columns\n",
    "            for c in cat_cols:\n",
    "                if self.data[c].isna().sum() > 0:\n",
    "                    mode_val = self.data[c].mode()[0] if not self.data[c].mode().empty else 'unknown'\n",
    "                    self.data[c] = self.data[c].fillna(mode_val)\n",
    "        self.feature_groups = feature_groups if feature_groups else self._infer_feature_groups()\n",
    "        self.X = self.data.drop(columns=[self.target_col])\n",
    "        self.y = self.data[self.target_col] if self.target_col in self.data.columns else pd.Series(dtype='int64')\n",
    "\n",
    "    def _infer_feature_groups(self) -> Dict[str, List[str]]:\n",
    "        cols = [c for c in self.data.columns if c != self.target_col]\n",
    "        groups = {\n",
    "            'Demographics': [c for c in cols if any(x in c.upper() for x in ['AGE','SEX','EDUCATION','MARRIAGE','DEMO'])],\n",
    "            'Utilization': [c for c in cols if any(x in c.upper() for x in ['UTIL','LIMIT'])],\n",
    "            'Payment_History': [c for c in cols if 'PAY_' in c],\n",
    "            'Payment_Amount': [c for c in cols if 'PAY_AMT' in c],\n",
    "            'Bill_Amount': [c for c in cols if 'BILL_AMT' in c],\n",
    "            'Payment_Behavior': [c for c in cols if any(x in c.upper() for x in ['DELAY','PAYMENT_RATIO','CONSISTENCY'])],\n",
    "            'Financial_Stress': [c for c in cols if any(x in c.upper() for x in ['STRESS','DEBT','SPIRAL'])],\n",
    "            'Trends': [c for c in cols if any(x in c.upper() for x in ['TREND','MOMENTUM','VOLATILITY','MA_','ACCEL'])],\n",
    "        }\n",
    "        categorized = sum(groups.values(), [])\n",
    "        groups['Other'] = [c for c in cols if c not in categorized]\n",
    "        return {k:v for k,v in groups.items() if v}\n",
    "\n",
    "    def plot_target_distribution(self, save: bool = True):\n",
    "        plt.figure(figsize=(10,6))\n",
    "        d = self.data.dropna(subset=[self.target_col]) if self.target_col in self.data.columns else self.data\n",
    "        ax = sns.countplot(x=self.target_col, data=d) if self.target_col in d.columns else None\n",
    "        if ax is not None:\n",
    "            total = len(d)\n",
    "            for p in ax.patches:\n",
    "                h = p.get_height(); ax.text(p.get_x()+p.get_width()/2., h*1.01, f'{100*h/total:.1f}%', ha='center')\n",
    "        plt.title('Distribution of Default vs Non-Default'); plt.xlabel('Default (1) vs Non-Default (0)'); plt.ylabel('Count')\n",
    "        if save: plt.tight_layout(); plt.savefig(self.output_dir / 'target_distribution.png', dpi=300); plt.close()\n",
    "\n",
    "    def plot_correlation_matrix(self, top_n: int = 20, save: bool = True):\n",
    "        correlations = self.data.corr(numeric_only=True)[self.target_col].sort_values(ascending=False) if self.target_col in self.data.columns else self.data.corr(numeric_only=True).iloc[:,0]\n",
    "        top_features = correlations.drop(self.target_col).abs().nlargest(top_n).index.tolist() if self.target_col in correlations.index else correlations.abs().nlargest(top_n).index.tolist()\n",
    "        selected = top_features + ([self.target_col] if self.target_col in self.data.columns else [])\n",
    "        corr_matrix = self.data[selected].corr(numeric_only=True)\n",
    "        plt.figure(figsize=(12,10)); mask = np.triu(np.ones_like(corr_matrix, dtype=bool)); cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "        sns.heatmap(corr_matrix, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0, square=True, linewidths=.5, cbar_kws={'shrink': .5}, annot=True, fmt='.2f')\n",
    "        plt.title(f'Correlation Matrix: Top {len(top_features)} Features')\n",
    "        if save: plt.tight_layout(); plt.savefig(self.output_dir / 'correlation_matrix.png', dpi=300); plt.close()\n",
    "\n",
    "    def plot_feature_importance(self, method: str = 'mutual_info', top_n: int = 20, save: bool = True) -> pd.Series:\n",
    "        X = self.X.copy();\n",
    "        num_cols = X.select_dtypes(include=['number']).columns\n",
    "        X[num_cols] = X[num_cols].fillna(X[num_cols].median())\n",
    "        cat_cols = X.select_dtypes(exclude=['number']).columns\n",
    "        for c in cat_cols: X[c] = X[c].fillna(X[c].mode()[0] if not X[c].mode().empty else 'unknown')\n",
    "        if method == 'mutual_info' and shap is not None:\n",
    "            try:\n",
    "                scores = mutual_info_classif(X.select_dtypes(include=['number']), self.y)\n",
    "                s = pd.Series(scores, index=X.select_dtypes(include=['number']).columns)\n",
    "            except Exception:\n",
    "                s = self.data.corr(numeric_only=True)[self.target_col].drop(self.target_col).abs()\n",
    "        else:\n",
    "            s = self.data.corr(numeric_only=True)[self.target_col].drop(self.target_col).abs()\n",
    "        s = s.sort_values(ascending=False).head(top_n)\n",
    "        plt.figure(figsize=(12,8)); s.plot(kind='barh'); plt.title(f'Top {len(s)} Features ({method})'); plt.xlabel('Importance'); plt.ylabel('Feature'); plt.grid(axis='x')\n",
    "        if save: plt.tight_layout(); plt.savefig(self.output_dir / f'feature_importance_{method}.png', dpi=300); plt.close()\n",
    "        return s\n",
    "\n",
    "    def generate_feature_report(self) -> None:\n",
    "        self.plot_target_distribution()\n",
    "        self.plot_correlation_matrix()\n",
    "        self.plot_feature_importance(method='mutual_info')\n",
    "        self.plot_feature_importance(method='correlation')\n",
    "        print(f'Feature visualizations saved to {self.output_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4f5e7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal MPI helpers (optional)\n",
    "def setup_mpi():\n",
    "    if MPI_AVAILABLE and ('OMPI_COMM_WORLD_RANK' in os.environ or 'PMI_RANK' in os.environ):\n",
    "        comm = MPI.COMM_WORLD; return comm, comm.Get_rank(), comm.Get_size()\n",
    "    return None, 0, 1\n",
    "\n",
    "def distribute_and_process(comm, rank, size, preprocessor: CreditCardPreprocessor):\n",
    "    if size <= 1:\n",
    "        return False  # not distributed\n",
    "    logger.info(f'Rank {rank}: starting distributed preprocessing')\n",
    "    if rank == 0:\n",
    "        data = preprocessor.load_data()\n",
    "        chunks = np.array_split(data, size)\n",
    "        local = chunks[0]\n",
    "        for i in range(1, size):\n",
    "            comm.send(chunks[i], dest=i)\n",
    "    else:\n",
    "        local = comm.recv(source=0)\n",
    "    preprocessor.raw_data = local\n",
    "    preprocessor.clean_data(); preprocessor.engineer_features()\n",
    "    if rank == 0:\n",
    "        all_parts = [preprocessor.processed_data]\n",
    "        for i in range(1, size):\n",
    "            all_parts.append(comm.recv(source=i))\n",
    "        preprocessor.processed_data = pd.concat(all_parts, ignore_index=True)\n",
    "    else:\n",
    "        comm.send(preprocessor.processed_data, dest=0)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49b4225d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 20:50:54,410 - WARNING - Limited memory (15.5 GB). Setting n_jobs=1 for stability.\n",
      "2025-08-24 20:50:54,411 - INFO - Starting preprocessing pipeline...\n",
      "2025-08-24 20:50:54,411 - INFO - Loading data from /home/satyakarthikeya/Desktop/Hpc/archive/UCI_Credit_Card.csv\n",
      "2025-08-24 20:50:54,411 - INFO - Starting preprocessing pipeline...\n",
      "2025-08-24 20:50:54,411 - INFO - Loading data from /home/satyakarthikeya/Desktop/Hpc/archive/UCI_Credit_Card.csv\n",
      "2025-08-24 20:50:54,446 - INFO - Data loaded: (30000, 25), mem ~ 1.95 MB\n",
      "2025-08-24 20:50:54,448 - INFO - Class 0: 77.88%\n",
      "2025-08-24 20:50:54,448 - INFO - Class 1: 22.12%\n",
      "2025-08-24 20:50:54,449 - WARNING - Invalid EDUCATION values: [2 1 3 5 4 6 0]\n",
      "2025-08-24 20:50:54,450 - WARNING - Found 590 negative values in BILL_AMT1\n",
      "2025-08-24 20:50:54,451 - WARNING - Found 669 negative values in BILL_AMT2\n",
      "2025-08-24 20:50:54,451 - WARNING - Found 655 negative values in BILL_AMT3\n",
      "2025-08-24 20:50:54,452 - WARNING - Found 675 negative values in BILL_AMT4\n",
      "2025-08-24 20:50:54,452 - WARNING - Found 655 negative values in BILL_AMT5\n",
      "2025-08-24 20:50:54,453 - WARNING - Found 688 negative values in BILL_AMT6\n",
      "2025-08-24 20:50:54,453 - INFO - Cleaning data...\n",
      "2025-08-24 20:50:54,446 - INFO - Data loaded: (30000, 25), mem ~ 1.95 MB\n",
      "2025-08-24 20:50:54,448 - INFO - Class 0: 77.88%\n",
      "2025-08-24 20:50:54,448 - INFO - Class 1: 22.12%\n",
      "2025-08-24 20:50:54,449 - WARNING - Invalid EDUCATION values: [2 1 3 5 4 6 0]\n",
      "2025-08-24 20:50:54,450 - WARNING - Found 590 negative values in BILL_AMT1\n",
      "2025-08-24 20:50:54,451 - WARNING - Found 669 negative values in BILL_AMT2\n",
      "2025-08-24 20:50:54,451 - WARNING - Found 655 negative values in BILL_AMT3\n",
      "2025-08-24 20:50:54,452 - WARNING - Found 675 negative values in BILL_AMT4\n",
      "2025-08-24 20:50:54,452 - WARNING - Found 655 negative values in BILL_AMT5\n",
      "2025-08-24 20:50:54,453 - WARNING - Found 688 negative values in BILL_AMT6\n",
      "2025-08-24 20:50:54,453 - INFO - Cleaning data...\n",
      "2025-08-24 20:51:03,927 - INFO - Top features by correlation with target:\n",
      "2025-08-24 20:51:03,927 - INFO -   DELAY_COUNT: 0.3984\n",
      "2025-08-24 20:51:03,928 - INFO -   DELAY_NORM: 0.3984\n",
      "2025-08-24 20:51:03,928 - INFO -   RECENT_DELAY_COUNT: 0.3964\n",
      "2025-08-24 20:51:03,929 - INFO -   AVG_DELAY: 0.3683\n",
      "2025-08-24 20:51:03,929 - INFO -   MAX_DELAY: 0.2935\n",
      "2025-08-24 20:51:03,930 - INFO -   PAY_0: 0.2917\n",
      "2025-08-24 20:51:03,930 - INFO -   FINANCIAL_STRESS_SCORE: 0.2777\n",
      "2025-08-24 20:51:03,930 - INFO -   PAY_2: 0.2353\n",
      "2025-08-24 20:51:03,931 - INFO -   PAY_3: 0.2086\n",
      "2025-08-24 20:51:03,931 - INFO -   PAY_4: 0.1870\n",
      "2025-08-24 20:51:03,927 - INFO - Top features by correlation with target:\n",
      "2025-08-24 20:51:03,927 - INFO -   DELAY_COUNT: 0.3984\n",
      "2025-08-24 20:51:03,928 - INFO -   DELAY_NORM: 0.3984\n",
      "2025-08-24 20:51:03,928 - INFO -   RECENT_DELAY_COUNT: 0.3964\n",
      "2025-08-24 20:51:03,929 - INFO -   AVG_DELAY: 0.3683\n",
      "2025-08-24 20:51:03,929 - INFO -   MAX_DELAY: 0.2935\n",
      "2025-08-24 20:51:03,930 - INFO -   PAY_0: 0.2917\n",
      "2025-08-24 20:51:03,930 - INFO -   FINANCIAL_STRESS_SCORE: 0.2777\n",
      "2025-08-24 20:51:03,930 - INFO -   PAY_2: 0.2353\n",
      "2025-08-24 20:51:03,931 - INFO -   PAY_3: 0.2086\n",
      "2025-08-24 20:51:03,931 - INFO -   PAY_4: 0.1870\n",
      "2025-08-24 20:51:04,047 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-08-24 20:51:04,047 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-08-24 20:51:04,058 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-08-24 20:51:04,058 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test shapes: (24000, 117) (6000, 117)\n",
      "Feature visualizations saved to /home/satyakarthikeya/Desktop/Hpc/processed_data/visualizations\n",
      "Feature visualizations saved to /home/satyakarthikeya/Desktop/Hpc/processed_data/visualizations\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline\n",
    "cfg = load_config()\n",
    "pre = CreditCardPreprocessor(\n",
    "    input_path=cfg.get('input_path', DEFAULT_INPUT),\n",
    "    output_dir=cfg.get('output_dir', DEFAULT_OUTPUT_DIR),\n",
    "    test_size=cfg.get('preprocessing',{}).get('test_size', 0.2),\n",
    "    random_state=cfg.get('preprocessing',{}).get('random_state', 42),\n",
    "    n_jobs=cfg.get('preprocessing',{}).get('n_jobs', -1),\n",
    "    memory_threshold=cfg.get('preprocessing',{}).get('memory_threshold', 0.8),\n",
    "    save_intermediates=cfg.get('preprocessing',{}).get('save_intermediates', True),\n",
    "    output_formats=cfg.get('preprocessing',{}).get('output_formats', ['csv','parquet']),\n",
    "    chunk_size=cfg.get('preprocessing',{}).get('chunk_size', 5000)\n",
    ")\n",
    "comm, rank, size = setup_mpi()\n",
    "distributed = distribute_and_process(comm, rank, size, pre) if size > 1 else False\n",
    "if (not distributed) or (rank == 0):\n",
    "    Xtr, Xte, ytr, yte = pre.process_pipeline()\n",
    "    print('Train/Test shapes:', Xtr.shape, Xte.shape)\n",
    "\n",
    "    # Generate visualizations on the combined training data\n",
    "    train_df = pd.concat([Xtr, ytr], axis=1)\n",
    "    viz = FeatureVisualization(train_df, target_col='default.payment.next.month', output_dir=str(Path(pre.output_dir)/'visualizations'))\n",
    "    viz.generate_feature_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cde450e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has process_pipeline: True\n",
      "methods containing \"process\": ['process_pipeline', 'save_full_processed']\n"
     ]
    }
   ],
   "source": [
    "# Debug: verify class has process_pipeline\n",
    "print('has process_pipeline:', hasattr(CreditCardPreprocessor, 'process_pipeline'))\n",
    "methods = [m for m in dir(CreditCardPreprocessor) if 'process' in m]\n",
    "print('methods containing \"process\":', methods)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monte_carlo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
